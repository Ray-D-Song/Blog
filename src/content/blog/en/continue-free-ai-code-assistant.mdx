---
title: 'Deploy a Free AI Code Assistant with Continue'
date: '2024-09-04'
cover: ''
tag: ['Cloudflare', 'AI']
---

Over the past year, I have been using GitHub Copilot as a code assistant, which provides features like autocompletion, Q&A, etc., for a subscription fee of $10/month.

To be honest, this price is not expensive, but recently Copilot has restricted the scope of Q&A, causing many development-related questions to go unanswered.

I also tried Codium AI later, but the response speed was unsatisfactory.

Recently, I discovered an open-source code assistant called [Continue](https://www.continue.dev/), which supports calling both local and cloud-based models, including Cloudflare's worker AI.

## Introduction
Continue is essentially a client that sends requests to large models on your behalf.
When you write code, it captures the context of the code, sends a request to the model, processes the result, waits for your command (tab), and then integrates it into the code.

## Local Deployment
For local deployment, you can use [Ollama](https://ollama.com/) or [lmstudio](https://docs.continue.dev/reference/Model%20Providers/lmstudio).
These are two very convenient tools for integrating large models, supporting Mac, Windows, and Linux, to help you quickly run large models locally.

Here, I choose to run Ollama on Mac. You can download the installation package from the Ollama official website. Remember to select the command-line tool during the installation process.

https://ollama.com/

### Configuration Requirements
The configuration requirements depend on the model you want to use. Here, llama3 is recommended.
llama3 is a meta open-source model and currently the most powerful open-source model, providing good autocompletion effects.
The model has two variants: 8B requires at least 16GB of memory and 8GB of VRAM, while 70B requires over 64GB of memory.
The default 8B version should suffice for normal needs.

### Running the Model
After installation, enter the terminal and run the following command to fetch the llama3 model:

```bash
ollama run llama3
```

Once the model is fetched, it will start running automatically.
Test by asking it to write a Python version of hello-world in Chinese.
![test](https://r2.ray-d-song.com/2024/09/6d3a74f153d36136f7ae81586e8315fb.png)

You can exit the running by typing `/bye`.

### Configuring the Continue Plugin
Install the [vscode plugin](https://marketplace.visualstudio.com/items?itemName=Continue.continue).

After installation, the left sidebar will display the Continue dialog:
![chat](https://r2.ray-d-song.com/2024/09/ea18b185908bda96720414fcad005f10.png)

You cannot chat at this point because you have not selected a model. Click `Select model` -> `Add Model`, scroll down, and find:
![Ollama](https://r2.ray-d-song.com/2024/09/58d456bc4ba21cacbb2f2c2b076722de.png)

Then select the first `Autodetect` option:
![model](https://r2.ray-d-song.com/2024/09/05191d0410e4f23c30285563b15c54dc.png)

Return to the chat and select the Ollama option:
![back to chat](https://r2.ray-d-song.com/2024/09/f82f1758e1ea1f85cf5cd9db3913e11d.png)

Ensure your model is running in the background, and you can start chatting.
![test chat](https://r2.ray-d-song.com/2024/09/2880fe2110c014bc1b6e736d41bb0e71.png)

### Autocompletion Settings
Although you can now chat, autocompletion is still not working.
![tab 404](https://r2.ray-d-song.com/2024/09/c072cad8863c2a50f9202e05e3442d44.png)
This is because the default autocompletion model is `Starcoder2 3b`, which is a model specifically designed for code scenarios.

You can start this model using the following command:
```bash
ollama run starcoder2
```

Once the model is running, autocompletion can be triggered.  

If your machine's performance is not sufficient to run two models simultaneously, you can also set up autocompletion using `llama3`:

In the corner of the plugin sidebar, find this small gear icon  
![config](https://r2.ray-d-song.com/2024/09/d3b06fcec439af8e2cec4d360f2663c2.png)

Click on it to enter the configuration file (or you can open ~/.continue/config.json), and change the values of the two highlighted lines in the `tabAutocompleteModel` field to `llama3`.  
```json
"tabAutocompleteModel": {
  "title": "Starcoder2 3b", // [!code highlight]
  "provider": "ollama",
  "model": "starcoder2:3b" // [!code highlight]
},
```

> If you want to autocomplete text files like markdown, I recommend using llama3 for autocompletion.

## Cloudflare worker AI
Some lightweight laptops may not be able to run large models locally, so you can use Cloudflare worker's AI model.

Cloudflare provides free quotas, measured in `neurons`, which can be estimated on [this page](https://ai.cloudflare.com/#pricing-calculator).

The daily free quota is roughly equivalent to 500 calls, with 100 input characters and 500 output characters per call, which is sufficient for normal development scenarios.

First, follow the [Cloudflare guide to create a key](https://developers.cloudflare.com/fundamentals/api/get-started/create-token/).

Then open the configuration file, add a `models` field value, and modify the `tabAutocompleteModel` field.

```json
{
  "models": [
    {
      "accountId": "YOUR CLOUDFLARE ACCOUNT ID",
      "apiKey": "YOUR CLOUDFLARE API KEY",
      "contextLength": 2400,
      "completionOptions": {
        "maxTokens": 500
      },
      "model": "@cf/meta/llama-3-8b-instruct",
      "provider": "cloudflare",
      "title": "Llama 3 8B"
    }
  ],
  "tabAutocompleteModel": {
    "accountId": "YOUR CLOUDFLARE ACCOUNT ID",
    "apiKey": "YOUR CLOUDFLARE API KEY",
    "model": "@hf/thebloke/deepseek-coder-6.7b-base-awq",
    "provider": "cloudflare",
    "title": "DeepSeek 7b"
  },
}
```

Change the accountId to your own account ID, and apiKey to the key you just saved.

Obtaining the account ID can be a bit tricky. You must have a domain under Cloudflare or a Worker. For details, refer to the official documentation and these two pages:
* [Official Documentation](https://developers.cloudflare.com/fundamentals/setup/find-account-and-zone-ids/)
* [Reference 1](https://docs.thorn.red/articles/6d89ow4vgfghgnn8)
* [Reference 2](https://community.cloudflare.com/t/where-can-i-find-my-account-id/164431)

You can modify the value of the `model` field as needed. Different models require different computational power, which will also affect the billing based on `neurons`. If you find the free quota insufficient, you can switch to a lower-tier model.

Click the link to see the complete list of models:  
https://developers.cloudflare.com/workers-ai/models

## Summary
llama3 has good conversational effects but is slower, and its code completion is a bit rudimentary.

starcoder2 is much faster, provides better autocompletion, but has weaker text processing capabilities.

I recommend using Cloudflare's llama3 model for conversations and running starcoder2 locally for autocompletion, which is a more optimal solution.  

